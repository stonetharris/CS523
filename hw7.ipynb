{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10376493 0.26978881 0.06225896 0.21790635 0.03112948 0.21790635\n",
      " 0.         0.04150597]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rmsnorm(input_tensor, weights):\n",
    "    rms_value = np.sqrt(np.mean(input_tensor**2))\n",
    "    normalized_tensor = input_tensor / rms_value\n",
    "    weighted_normalized_tensor = weights * normalized_tensor\n",
    "    return weighted_normalized_tensor\n",
    "\n",
    "# input_a = np.array([5, 8, 7, 9, -2, 1, 0, 3])\n",
    "# weights_g = np.array([0.1, 0.2, 0.05, 0.15, 0.1, 0.35, 0, 0.05])\n",
    "# input_a = np.array([1, 2, 3, 4, 5])\n",
    "# weights_g = np.array([0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "\n",
    "input_a = np.array([5, 8, 7, 9, -2, 1, 0, 3]) + np.array([5, 5, 5, 5, 5, 5, 5, 5])\n",
    "weights_g = np.array([0.1, 0.2, 0.05, 0.15, 0.1, 0.35, 0, 0.05])\n",
    "\n",
    "output = rmsnorm(input_a, weights_g)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.26623136, 1.26991938, 0.80545367, 0.6829949 ],\n",
       "       [1.24198908, 1.24302783, 0.79473757, 0.67081707],\n",
       "       [1.26720979, 1.26977609, 0.80499216, 0.68366085],\n",
       "       [1.2262123 , 1.2267871 , 0.78715785, 0.66283928],\n",
       "       [1.27206669, 1.28105231, 0.8091645 , 0.68552159],\n",
       "       [1.27234947, 1.27676819, 0.80763497, 0.68616184],\n",
       "       [1.26686396, 1.26917116, 0.80513724, 0.68354414],\n",
       "       [1.23808574, 1.23624494, 0.79200041, 0.66916825],\n",
       "       [1.24328208, 1.24271685, 0.79397839, 0.67173777],\n",
       "       [1.28108688, 1.28503553, 0.81151524, 0.69063169]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def attention(x):\n",
    "    n, d = x.shape\n",
    "    Wq = np.random.rand(d, d)\n",
    "    Wk = np.random.rand(d, d)\n",
    "    Wv = np.random.rand(d, d)\n",
    "    \n",
    "    #compute queries, keys, values\n",
    "    Q = np.dot(x, Wq)\n",
    "    K = np.dot(x, Wk)\n",
    "    V = np.dot(x, Wv)\n",
    "    \n",
    "    #calculate attention scores\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d)\n",
    "    \n",
    "    #apply softmax to get the attention weights\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n",
    "    \n",
    "    #weighted sum of values\n",
    "    attention_output = np.dot(attention_weights, V)\n",
    "    return attention_output\n",
    "\n",
    "x = np.random.rand(10, 4)\n",
    "output = attention(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[152.89310592 139.01540984 157.00616999 154.17458983 179.12071845\n",
      "  152.46563544 129.33416633 182.86497666 145.52328195 138.73135416\n",
      "  166.82758555 129.94774183 152.46991469 163.08813098 136.50878431\n",
      "  150.89314796 163.39261797 151.28590963 150.33222748 165.26774751\n",
      "  156.21940788 143.07852477 152.49322381 104.0193102  169.14100661\n",
      "  133.17807781 181.03874156 182.5139274  156.55275881 175.23870587\n",
      "  149.81521697 139.19813471]\n",
      " [152.84967079 138.98997584 156.95389389 154.13447594 179.07781391\n",
      "  152.4115376  129.29671843 182.79998179 145.49224186 138.6870352\n",
      "  166.7803389  129.92286407 152.42832218 163.05047294 136.47580306\n",
      "  150.86375085 163.35437332 151.24672399 150.27972855 165.23473987\n",
      "  156.14997015 143.02152614 152.43052441 103.9976183  169.07909279\n",
      "  133.13594741 180.98906365 182.43985628 156.52009411 175.21660646\n",
      "  149.77236075 139.13260876]\n",
      " [153.06968073 139.1587932  157.24710103 154.30725075 179.23214561\n",
      "  152.75194558 129.48642369 183.11675559 145.66264885 138.99551423\n",
      "  167.0781936  130.05153285 152.66214678 163.36117218 136.70858401\n",
      "  151.14507574 163.5941422  151.51627829 150.58703312 165.57921979\n",
      "  156.38851006 143.3438703  152.72586823 104.15644245 169.3561707\n",
      "  133.30944305 181.25708108 182.8678519  156.73382882 175.39018608\n",
      "  150.04470642 139.44240891]\n",
      " [153.086304   139.00359505 157.17640242 154.27557318 179.14529661\n",
      "  152.62410803 129.38326317 183.03759096 145.61585816 138.85459749\n",
      "  166.94146335 130.02437175 152.50903614 163.27965593 136.65559301\n",
      "  150.99499344 163.469862   151.47083315 150.55631534 165.50080052\n",
      "  156.36896033 143.22976026 152.58956474 104.16878244 169.2711562\n",
      "  133.25520079 181.09864975 182.69693016 156.59604627 175.34894335\n",
      "  150.02198301 139.32035559]\n",
      " [153.02316701 138.99273765 157.09332084 154.24971257 179.14944249\n",
      "  152.5371662  129.36826147 182.94669607 145.58831008 138.76871249\n",
      "  166.86675925 130.01125934 152.49313581 163.18120487 136.58626752\n",
      "  150.94884633 163.43802836 151.37899948 150.42949364 165.3737209\n",
      "  156.31473166 143.16459782 152.51932068 104.08723142 169.21957731\n",
      "  133.23954793 181.06547429 182.60496324 156.57760137 175.32127017\n",
      "  149.9501766  139.22086079]\n",
      " [152.98182294 139.07517427 157.11159781 154.22880813 179.18384865\n",
      "  152.56180184 129.39271166 182.98252656 145.58638085 138.81796857\n",
      "  166.91975766 130.00196323 152.54467853 163.19446048 136.59795427\n",
      "  150.99323032 163.47936617 151.37459678 150.44357342 165.40100961\n",
      "  156.29516882 143.1938166  152.56709899 104.08271573 169.24365411\n",
      "  133.24865617 181.12213301 182.653582   156.63340214 175.33196443\n",
      "  149.92950631 139.29700171]\n",
      " [153.09420754 139.03857324 157.20553686 154.29511097 179.16578959\n",
      "  152.68049132 129.41539805 183.07362212 145.63275454 138.91284085\n",
      "  166.99617057 130.02347409 152.55865181 163.31499437 136.68169164\n",
      "  151.03617623 163.50269344 151.50212327 150.58738737 165.53320207\n",
      "  156.39877013 143.27913739 152.66132388 104.17726697 169.30970681\n",
      "  133.26673219 181.14615156 182.76196429 156.62973754 175.34294121\n",
      "  150.03574619 139.37080435]\n",
      " [152.97727854 139.03805592 157.08247291 154.22097334 179.16346312\n",
      "  152.52263237 129.36993699 182.94535593 145.57401789 138.77346336\n",
      "  166.87521717 130.00027938 152.50735446 163.16574559 136.57458077\n",
      "  150.95837    163.4490139  151.35220419 150.41190733 165.36554264\n",
      "  156.27765249 143.15493049 152.51798789 104.07127937 169.21118058\n",
      "  133.23578803 181.08222074 182.60138482 156.60155169 175.3240785\n",
      "  149.91629788 139.24509689]\n",
      " [153.01256882 139.04092145 157.10415542 154.23902202 179.18251558\n",
      "  152.53691674 129.3820735  182.96597831 145.59676359 138.77802916\n",
      "  166.88724358 130.01892888 152.52622778 163.18978064 136.59887153\n",
      "  150.98060611 163.47010631 151.36820178 150.43069222 165.39212173\n",
      "  156.30245654 143.18904476 152.52691526 104.08187406 169.2364655\n",
      "  133.2563491  181.09333185 182.6289994  156.61499755 175.35287312\n",
      "  149.95287151 139.24472842]\n",
      " [153.00551667 138.99896433 157.06531677 154.23837468 179.18264403\n",
      "  152.47253263 129.3653055  182.92372671 145.57967822 138.70456213\n",
      "  166.82272715 130.02505161 152.48926094 163.12244163 136.55654303\n",
      "  150.92769682 163.4356396  151.32641651 150.36944999 165.31243239\n",
      "  156.30173128 143.14346996 152.46125854 104.05114091 169.22089085\n",
      "  133.26336208 181.0573001  182.56422735 156.59046629 175.35098373\n",
      "  149.93922323 139.18329998]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    d_k = Q.shape[1]\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "    weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    return np.dot(weights, V)\n",
    "\n",
    "def multi_head_attention(x, head_n=16):\n",
    "    n, d = x.shape\n",
    "    assert d % head_n == 0, \"Dimension must be divisible by the number of heads\"\n",
    "    d_k = d // head_n\n",
    "    \n",
    "    #initialize the weights\n",
    "    Wq = np.random.rand(d, d)\n",
    "    Wk = np.random.rand(d, d)\n",
    "    Wv = np.random.rand(d, d)\n",
    "    \n",
    "    #linear transformations\n",
    "    Q = np.dot(x, Wq).reshape(n, head_n, d_k)\n",
    "    K = np.dot(x, Wk).reshape(n, head_n, d_k)\n",
    "    V = np.dot(x, Wv).reshape(n, head_n, d_k)\n",
    "    \n",
    "    heads = np.concatenate([scaled_dot_product_attention(Q[:, i, :], K[:, i, :], V[:, i, :]) for i in range(head_n)], axis=-1)\n",
    "    \n",
    "    #another linear transformation for the output\n",
    "    Wo = np.random.rand(d, d)\n",
    "    output = np.dot(heads, Wo)\n",
    "    \n",
    "    return output\n",
    "\n",
    "x = np.random.rand(10, 32)\n",
    "output = multi_head_attention(x)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
